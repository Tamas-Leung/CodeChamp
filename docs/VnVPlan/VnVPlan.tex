\documentclass[12pt, titlepage]{article}
\usepackage{longtable}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{lscape} 

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
April 3, 2023 & 1.1 & Updated test cases, removed unnecessary test cases. Added new test cases for new requirements. Added usability section and questions. Added Unit testing section. Added Tracability matrix for functional requirements. \\
\midrule
November 1, 2022 & 1.0 & Added initial version.\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}

\begin{table}[H]
\begin{center}
\begin{tabular}{|p{3cm}|p{9cm}|}
\hline
  \textbf{Symbol} & \textbf{Description}\\
  \hline
  Data Structures and Algorithms & A topic of study for Computer Scientists.\\
  \hline
  CodeChamp & The system being built and tested.\\
  \hline
  Angular & A web framework for building web applications.\\
  \hline
  JavaScript & A programming language that can be executed by browsers.\\
  \hline
  TypeScript & A syntactical superset of JavaScript.\\
  \hline
  WebSocket & A communications protocol used for two-way interaction.\\
  \hline
  Client & A device used to connect to a CodeChamp instance.\\
  \hline
  DSA & Abbreviation for Data Structures and Algorithms.\\
  \hline
  CI & Abbreviation for Continuous Integration.\\
  \hline
  SRS & Abbreviation for Software Requirements Specification.\\
  \hline
  MIS & Abbreviation for Module Interface Specification.\\
  \hline
  MG & Abbreviation for Module Guide.\\
  \hline
  TestCafe & An end-to-end testing framework for web applications. \\
  \hline
  VSCode & A light weight code editor.\\
  \hline
  Mocha & A JavaScript testing framework.\\
  \hline
  Jasmine &  A JavaScript testing framework.\\
  \hline
  Git & A version control system for software.\\
  \hline
  Husky & A tool used to setup Git hooks.\\
  \hline
\end{tabular}
\end{center}
\caption{Symbols, Abbreviations and Acronyms}            

\end{table}

\newpage

\pagenumbering{arabic}

% This document ... \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}

\section{General Information}

\subsection{Summary}

This documentâ€™s purpose is to show the detailed testing plan for the game
CodeChamp. This document contains sections regarding information on the
test plans, system test descriptions (functional and non-function require-
ments) and the unit testing plans. It explains how tests will be performed
with details about initial state, input and output. This document will include
descriptions of all the testing, validating and verification procedures.

\subsection{Objectives}

  The objective of the test plan is to test the functionality of CodeChamp. The
final goal of the test plan is to show that all the functional requirements and
the non functional requirements from the software requirements specification are met.
This way, we can demonstrate adequate usability and build confidence in the correctness of our software.

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (MG, MIS, etc).  You can include these even
%   before they are written, since by the time the project is done, they will be
%   written.}

\begin{enumerate}
    \item \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/docs/DevelopmentPlan}{Development Plan}
    \item \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/docs/SRS}{System Requirements Specification} 
    \item \href{https://github.com/Tamas-Leung/CodeChamp/blob/main/docs/HazardAnalysis/HazardAnalysis.md}{Hazard Analysis} 
    \item \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/docs/Design/MIS}{Module Interface Specification} 
    \item \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/docs/Design/MG}{Module Guide} 
\end{enumerate}

\section{Plan}

% \wss{Introduce this section.   You can provide a roadmap of the sections to
%   come.}

This section outlines the verification and validation team for CodeChamp alongside their roles. Additionally, it describes how different components will be tested, including the SRS, design and implementation. Finally, it specifies the testing and verification tools that will be used to accomplish that.

\subsection{Verification and Validation Team}

% \wss{You, your classmates and the course instructor.  Maybe your supervisor.
%   You shoud do more than list names.  You should say what each person's role is
%   for the project.  A table is a good way to summarize this information.}


\begin{table}[h]
\begin{center}
\begin{tabular}{|l | l|}
\hline
  \textbf{Member} & \textbf{Role}\\
  \hline
  Anton Kanugalawattage & Automatic verification of front-end code; Manual code review\\
  \hline
  Dipendra Subedi & End-to-End testing; Manual SRS verification\\
  \hline
  Youssef Rizkalla & Automatic verification of back-end code; Manual code review\\
  \hline
  Tamas Leung & Performance testing; End-to-End testing\\
  \hline
  Zhiming Zhao & Integration of CI actions; Manual SRS verification\\
  \hline
  Spencer Smith & Manual review of documents and system\\
  \hline
  Chris Schankula & Manual review of documents and system\\
  \hline
  Classmate Review Groups & Manual review of documents and system\\
  \hline
\end{tabular}
\end{center}
\caption{Verification and Validation Team Members and Roles}            

\end{table}

All team members will participate in testing features all around the tech stack. The roles are intended for each member to have a focus area, which can later change depending on the team's needs. If a team member finishes early, they can help other team members with another part of the testing. Likewise, if a testing area is more labour-intensive than initially expected, other team members can spend additional time helping with that testing area.

\subsection{SRS Verification Plan}

% \wss{List any approaches you intend to use for SRS verification.  This may just
%   be ad hoc feedback from reviewers, like your classmates, or you may have
%   something more rigorous/systematic in mind..}
The SRS checklist will be utilized to ensure that all requirements are verifiable. In turn, system tests will be derived to ensure that all functional and non-functional requirements are verified. Automated tests will be done when possible, using the end-to-end testing framework TestCafe. This will simulate the experience of a real user interacting with the system and will allow developers to easily identify regressions. Parts of the system that may be difficult to test this way will be tested and/or reviewed manually by the project developers as well as members of the testing team.

% \wss{Remember you have an SRS checklist}

\subsection{Design Verification Plan}

The MG and MIS checklists will ensure that the modules specified in the MIS and MG are designed with high quality. Additionally, the designs individually architected by each engineer will be reviewed by the other engineers on the team. Finally, the design will be peer-reviewed by engineers belonging to other capstone groups, as well as the instructors. Feedback from peers and instructors will be considered in the final iteration of the design.

% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Remember you have MG and MIS checklists}

\subsection{Implementation Verification Plan}

Outlined system tests will be used to verify that the implementation of the system meets the requirements. Unit tests will be expected alongside each code change to the main branch of the repository. Each pull request will be reviewed by at least 2 other engineers. During each code walk-through/review the implemented changes will be verified by the reviewers. Additionally, the CI will automatically run tests against each new pull request to verify that it passes all unit tests and to help developers identify regressions in code coverage.

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static verification of
%   the implementation.  Potential techniques include code walkthroughs, code
%   inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

\begin{itemize}
    \item Programming Language: JavaScript
    \item Testing Frameworks:
   \begin{itemize}
       \item Backend (Unit-Tests): Mocha
       \item Frontend (Unit-Tests): Jasmine
       \item End-to-End: TestCafe
   \end{itemize}
   \item Code Linting / Formatting / Style:
   \begin{itemize}
       \item Linter: ESLint
       \item Formatter: Prettier
        \item The \href{https://github.com/airbnb/javascript}{Airbnb style guide} will be enforced during code review for all back-end code
        \item The \href{https://angular.io/guide/styleguide}{official Angular style guide} will be enforced during code review for all front-end code
       \item Husky pre-commit hooks will be used to automatically apply linters and formatters before pushing to the remote repository
   \end{itemize}
\item GitHub Actions CI 
    \begin{itemize}
        \item Auto build and test new pull requests using aforementioned testing frameworks
        \item Test new pull requests for linting
        \item Test new pull requests for formatting
    \end{itemize}
\item Code Coverage
    \begin{itemize}
        \item All testing frameworks support options for providing code coverage
        \item GitHub Actions CI will state code coverage on new pull requests and report regressions
        
    \end{itemize}

\item Performance Testing: 
    \begin{itemize}
       \item Backend: Postman Performance Testing
       \item Frontend: Chrome Dev Tools Performance Tester
   \end{itemize}
\end{itemize}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

\subsection{Software Validation Plan}

 An internal software validation will be done by the developers weekly by reviewing the changes that were made and by doing a walk-through of the system. This will allow the stakeholders (developers in the team) to validate that the implementation of the system meets the desired needs. To ensure that the platform is helpful to the target demographic, a survey will be conducted with user groups. Section \ref{usability} details how these sessions will be conducted.


\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good.}

\subsubsection{Match Making Tests}

% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{}

\begin{enumerate}

\item{TC-MM-1: Join a random match}

Testing Type: Functional, Manual, Dynamic
					
Initial State: On Main Menu Page
					
Input: Press the ``find a match" button
					
Output: Screen changes to the lobby page

Test Case Derivation: The system receives the ``find a match" input and returns the user a lobby match id to join, which sends the user to the lobby page.
					
How test will be performed: Tester will be used to find and press the ``find a match" button, wait until the HTML page is loaded, and then checks the HTML of the page to ensure the lobby page is displayed.
					
\item{TC-MM-2: Join an existing page with a code}

Testing Type: Functional, Manual, Dynamic
					
Initial State: On Main Menu Page
					
Input: Inputs an existing lobby's code and presses the ``join match" button.
					
Output: Screen changes to lobby page.

Test Case Derivation: The system receives the ``join match" input with a code and returns the user the lobby match that matches the inputted code, which sends the user to the lobby page.
					
How test will be performed: Tester creates a lobby with one machine, receives the code from the screen, then uses the code to join the match. Tester then checks to see if the page changes to the lobby page and if the lobby code matches the inputted lobby code.

\item{TC-MM-3: Create a match}

Testing Type: Functional, Manual, Dynamic 
					
Initial State: On Main Menu Page
					
Input: Press the ``create match" button.
					
Output: Screen changes to lobby page.

Test Case Derivation: The system receives the ``create match" input with a code and returns the user the lobby match with a new code, which sends the user to the lobby page.
					
How test will be performed: Tester will be used to press the ``create match" button, wait until the HTML page is loaded, and then checks the HTML of the page to ensure the lobby page is displayed.

\end{enumerate}

\subsubsection{In-Game Tests}

\begin{enumerate}

\item{TC-IG-1: Complete a full match}

Testing Type: Functional, Manual, Dynamic
					
Initial State: Start of In-game Page
					
Input: Enter the correct code and submit the code in NUMBER\_OF\_ROUNDS rounds.
					
Output: After every successful submission code input, a new problem is displayed. After the last round's successful submission, a win-end game page is displayed.

Test Case Derivation: The system receives the submitted code input with user-written code. The system then validates and allows the user to move to the next round. There are only NUMBER\_OF\_ROUNDS rounds per game, on the last round, the game ends and gives the user a win.
					
How test will be performed: A user will start a game. The user will manually find out the answers for each question and inputs them. The user will submit every round and ensure a new problem is displayed when the next round starts or the win end game screen appears if it was the last round. The user also counts to ensure the number of rounds played is equal to NUMBER\_OF\_ROUNDS.

\item{TC-IG-2: Compiles code}

Testing Type: Functional, Manual, Dynamic
					
Initial State: In-game Page
					
\end{enumerate} 

Input: The following code snippet with the JavaScript language option:
\begin{verbatim}
console.log(`Hello, World!');
\end{verbatim}
					
Output: Returns success status and presents the user with the number of passed test cases.

Test Case Derivation: The system receives the submitted code input with user-written code. The system then validates and compiles the code. The system then returns the number of passed test cases.
					
How test will be performed: Tester will type the input code into a pre-determined Hello World problem and input it into the code editor. Tester will choose the JavaScript language option from a dropdown menu. The tester waits until the HTML page is loaded and checks the HTML to ensure that all test cases were passed.

\subsubsection{Problem Maintenance Tests}
\begin{enumerate}
\item{TC-PM-1: Add a new problem}

Testing Type: Functional, Manual, Dynamic
					
Initial State: problem maintenance page
					
Input: Input a new problem with the following data and click ``Add Problem" 
\begin{verbatim}
{
    name: "Hello World Test Problem",
    description: "Data"
}
\end{verbatim}
					
Output: List of problems on the problem maintenance page shows the new problem with the new name and description

Test Case Derivation: The system receives the new problem and updates the database. The system then updates the display to show the new problem.
					
How test will be performed: Tester will type the test problem data and click ``Add Problem". Tester will check the list of problems displayed including the new problem.

\item{TC-PM-2: Modify a problem}

Testing Type: Functional, Manual, Dynamic
					
Initial State: problem maintenance page
					
Input: Clicks ``Edit" button onto a problem and edits the problem name to ``Test Problem Name".
					
Output: A list of problems on the problem maintenance page shows the problem with the name ``Test Problem Name".

Test Case Derivation: The system receives the new problem name and updates the database. The system then updates the display to show the updated problem name.
					
How test will be performed: Tester will click edit on a test problem and edit the problem name. Tester will check the list of problems displayed including the new problem name.

\item{TC-PM-3: Delete a problem}

Testing Type: Functional, Tester, Dynamic
					
Initial State: problem maintenance page
					
Input: Clicks ``Delete" button onto a problem
					
Output: The list of problems on the problem maintenance page does not show the problem

Test Case Derivation: The system receives the delete problem and updates the database. The system then updates the display to show the problem is gone from the list.
					
How test will be performed: Tester will click delete on a test problem. Tester will check the list of problems displayed does not include the deleted test problem.
\end{enumerate}

\subsubsection{Profile View Tests}

%%FR28, FR27, FR26, FR25 

\begin{enumerate}
\item {TC-PV-1: Profile View Displays Win Percentage}

Testing Type: Functional, Manual, Dynamic
					
Initial State: Profile View Page
					
Input: Clicks a button to view their match statistics.
					
Output: Page displays the number of matches the player has played, the number they have won as well as their win percentage.

Test Case Derivation: The system retrieves the match statistics from the database and sends them to the client. The system then calculates and displays the appropriate statistics for the player.
					
How test will be performed: Tester will run in a special environment with mock data which includes several matches. Tester will check the HTML for an element which contains the win percentage number and ensure it matches the expected value in accordance with the mock data.

\item {TC-PV-2: Profile View Displays Match History}

Testing Type: Functional, Manual, Dynamic
					
Initial State: Profile View Page
					
Input: Clicks a match on the page.
					
Output: Match history displays the match result, the result for each round in that match and the coding problem for each match.

Test Case Derivation: The system retrieves the match statistics from the database and sends it to the client. The system then displays the appropriate statistics for a match which the user specified.
					
How test will be performed: Tester will run in a special environment with mock data which includes several matches. Tester will click on the first match on the list and check the HTML to ensure that the match data is displayed correctly in accordance with the provided mock data.

\end{enumerate}

\subsubsection{Leader-board Tests}


\begin{enumerate}
    \item {TC-LB-1}
    Testing Type: Functional, Manual, Dynamic
					
Initial State: Home Page
					
Input: Clicks the leader-board tab.
					
Output: The system displays up to 100 users, sorted by their number of wins in descending order.

Test Case Derivation: The system receives the leader board input and queries the database for the top 100 users by based on the number of wins of each user. The system then displays the results from the query.
					
How test will be performed: Tester will run in a special environment with mock data which includes multiple users with a different number of wins. Tester will click on the leaderboard tab. Tester will check HTML for each leader-board placement element to ensure that it is in the correct location in accordance with the mock data.
\end{enumerate}

\subsubsection{Authentication Tests}


\begin{enumerate}
    \item {TC-A-1}
    Testing Type: Functional, Manual, Dynamic
					
Initial State: Login Page
					
Input: Clicks the login button. Select an account to login with and put in the correct credentials.
					
Output: The system switch's to Home Page signalling a successful login.

Test Case Derivation: The System receives the login information then checks the authentication service and when successful redirects the user to the Home Page.
					
How test will be performed: Tester will click on login button. Tester will input their correct credentials.

 \item {TC-A-2}
    Testing Type: Functional, Manual, Dynamic
					
Initial State: Login Page
					
Input: Clicks the login button. Select an account to login with and put in the wrong credentials.
					
Output: The system outputs an error message saying the wrong credentials preventing them from logging in.

Test Case Derivation: The System receives the login information then checks the authentication service and when failure, returns an error message.
					
How test will be performed: Tester will click on the login button. Tester will input the wrong credentials. Tester checks to see if the error message appears.
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Look and Feel}

\paragraph{Interface Navigability}

\begin{enumerate}
\item{TC-LF-1: Interface should be easy to navigate and consistent}

Type: Non-Functional, Manual, Static
					
Initial State: Initial state of the game
					
Input/Condition: User actions
					
Output/Result: Completed game
					
How test will be performed: The test will be performed manually by a new user. They will have to navigate through the system to join a game and complete a game and will score the navigation on a scale out of 10 based on how easy they found it to navigate. A minimum of 8/10 should be achieved. If this is not achieved, we will implement the user's feedback and  re-run it on a new user.
\end{enumerate}

\paragraph{Screen Size Compatibility}

\begin{enumerate}
\item{TC-LF-2: Problem description and code editor should be fully visible on all screen sizes}

Type: Non-Functional, Manual, Dynamic
					
Initial State: The game screen is presented to the user
					
Input/Condition: The screen is viewed on devices (simulated or real) with different screen sizes
					
Output/Result: The description and editor must be fully visible on each screen size and aspect ratio tested without the need for scrolling the view
					
How test will be performed: The test will be performed manually by a member of the team. They will utilize Google Chrome's dev tools to simulate various aspect ratios and screen sizes.
\end{enumerate}


\subsubsection{Performance}

\paragraph{Maximum Action Latency}

\begin{enumerate}
\item{TC-P-1: The server should synchronize the state of the game to all clients in no more than INPUT\_RESPONSE\_TIME seconds}

Type: Non-Functional, Performance, Manual, Dynamic,
					
Initial State: The game is started and in its initial state
					
Input/Condition: The user sends actions to the server using various buttons and controls
					
Output/Result: The server should receive, process and synchronize those actions across all connected clients in no more than INPUT\_RESPONSE\_TIME seconds.
					
How test will be performed: The test will be performed manually by a member of the team. They will open Chrome's Dev Tools and record themselves performing the actions and getting the results back. They will then profile the recorded data to verify the time constraint was met.
\end{enumerate}

\paragraph{Maximum Compiling Latency}

\begin{enumerate}
\item{TC-P-2: The server should compile a solution and return results in no more than BACKEND\_COMPILE\_TIME seconds}

Type: Non-Functional, Performance, Manual, Dynamic,
					
Initial State: The game is started and in its first round

Input/Condition: The user sends a solution to the server.
					
Output/Result: Compiled results of the solution returned BACKEND\_COMPILE\_TIME seconds.
					
How test will be performed: The test will be performed manually by a member of the team. They will open Chrome's Dev Tools and record themselves performing the actions and getting the results back. They will then profile the recorded data to verify the time constraint was met.
\end{enumerate}

\subsubsection{Operational and Environmental}

\paragraph{Browser and Device Compatibility}

\begin{enumerate}

\item{TC-OE-1: The game should be able to be viewed and run on any modern browser}

Type: Compatibility, Manual, Dynamic
					
Initial State: The game is not running or being displayed on the browser window
					
Input/Condition: The game website is accessed on various modern browsers
					
Output/Result: The game should load all dependencies correctly and render the game properly
					
How test will be performed: The test will be performed manually by a team member. They will run the game site on the most commonly used modern browsers on multiple devices. They will note if any errors occur.

\end{enumerate}

\subsubsection{Security}

\paragraph{User authenticity}

\begin{enumerate}
\item{TC-S-1: The server must only accept input from verified clients to update the game}

Type: Security, Manual, Dynamic
					
Initial State: The game is started and is in any valid state of play
					
Input/Condition: The server receives an action from a simulated WebSocket connection from a client that is not logged in or not part of the game
					
Output/Result: The server blocks the connection and ignores any requests sent from it
					
How test will be performed: The test will be performed manually by a team member. They will create a WebSocket session outside of the game client and send events to the server. They will note whether or not those actions affected the game.
\end{enumerate}

\subsubsection{Cultural}

\paragraph{Content Moderation}
\begin{enumerate}
\item{TC-C-1: The system must not have any culturally inappropriate content}

Type: Manual, Static
					
Initial State: Current state of the codebase
					
Input/Condition: Snapshot of the current codebase 
					
Output/Result: True or False that content is verified not to be culturally inappropriate.
					
How test will be performed: The test will be performed manually by a team member. They will review the code snapshot and will find any cultural references. If there is at least one cultural reference the test fails. Based on the findings, cultural references are to be removed.
\end{enumerate}

\subsubsection{Legal}

\paragraph{Legal protection}
\begin{enumerate}
\item{TC-L-1: The system repository is protected by the GNU General Public License}

Type: Manual, Static
					
Initial State: Current state of the codebase
					
Input/Condition: System repository
					
Output/Result: True or False that the repository is protected by the GNU General Public License.
					
How test will be performed: The test will be performed manually by a team member. They will review the repository and ensure that legal protection is present.
\end{enumerate}

\subsubsection{Health and Safety}

\paragraph{Epilepsy shock test}
\begin{enumerate}
\item{TC-HS-1: The system has no flashing lights that could cause harm to the user}

Type: Manual
					
Initial State: The game is started and is in its initial state
					
Input/Condition: System interface
					
Output/Result: True or false that the system did not cause epilepsy.
					
How test will be performed: The test will be performed manually by a team member. The member will navigate the site and ensure that there are no flashing lights that could result in an epilepsy shock.
\end{enumerate}

\subsubsection{Usability Testing}\label{usability}

A semi-structured interview will be conducted with users in the target demographic. To achieve this, a CodeChamp lobby will be setup with a user group. Each session should target around 8-20 users participating in a CodeChamp game. After each user participates in a game, they will be asked several questions. The questions that will be asked can be found in the Appendix Section \ref{UsabilitySurveyQuestions}. As our platform's goal is to help users code, this survey will be used to evaluate the user's experience using the software, to find out if it's helpful in its current state and to determine potential new features that could be valuable to the  users.


\subsection{Dependencies Among Tests}
Some tests cannot be performed until other parts of the testing plans are completed. Figure \ref{fig:deps} demonstrates such dependencies, with an arrow from a test to another representing that the test cannot be performed until the test it points to has been performed.

\begin{figure}[H]
    \centering
    \includesvg[width=\textwidth]{deps.svg}
    \caption{A dependency graph among tests in the CodeChamp system}
    \label{fig:deps}
\end{figure}




\subsection{Traceability Between Test Cases and Requirements}


\begin{longtable}{| p{2.5cm} | p{3cm} | p{8cm}| }
    \hline
    Test Case ID & Requirement ID & Requirement Description\\
    \hline
    TC-MM-1 & FR.1 & Should join a random match.\\
    \hline
    TC-MM-2 & FR.19 & Should join an existing page with a code. \\
     \hline
    TC-MM-3 & FR.18 & Should create a match. \\
     \hline
    TC-IG-1 & FR.2, FR.3, FR.4, FR.5, FR.8, FR.9, FR.10 & Should complete a match. \\
     \hline
    TC-IG-2 & FR.6, FR.7, FR.15, FR.16, FR.17 & Should compile solutions from the input.\\
     \hline
    TC-PM-1 & FR.11, FR.12, FR.16, FR.17 & Should be able to add new problems (admin).\\
     \hline
    TC-PM-2 & FR.11, FR.12, FR.16, FR.17 & Should be able to modify problems (admin).\\
     \hline
    TC-PM-3 & FR.11, FR.12, FR.16, FR.17 & Should be able to delete problems (admin).\\
     \hline
    % TC-LS-1 & FR.17 & New users can sign up. \\
    %  \hline
    % TC-LS-2 & FR.18  & Existing users can log in. \\
    %  \hline
    % TC-LS-3 & FR.19  & Login with invalid information is rejected and a error message is displayed. \\
     % \hline
    TC-PV-1 & FR.21 & Profile should view win percentage.\\
    \hline
    TC-PV-2 & FR.20, FR.22, FR.23  & Profile should view match history.\\
    \hline
    TC-LB-1 & FR.24 & Leaderboard view should show the players sorted by their scores.\\
    \hline
    TC-A-1 & FR.13 & Users should be able to login\\
    \hline
    TC-A-2 & FR.14 & System should error when login credentials are wrong\\
    \hline
    TC-LF-1 & NFR.1, NFR.2, NFR.3 & Ease of navigation.\\
    \hline
    TC-LF-2 & NFR.2, NFR.3  & Responsive display among devices.\\
    \hline
     TC-P-1 & NFR.4 & Performance of user actions.\\
    \hline
    TC-P-2 & NFR.10 & Performance of user solution compilation.\\
     \hline
    TC-OE-1 & NFR.6 & Should run on any modern browser on any device. \\
     \hline
    TC-S-1 & NFR.7 & Only requests from authenticated users should be accepted. \\
     \hline
    TC-C-1 & NFR.10 & Content of the system should not contain any cultural references.\\
     \hline
    TC-L-1 & NFR.11 & System should be protected by GNU License.\\
     \hline
    % \end{tabularx}
    \caption{Traceability Table for Test Cases and Requirements}
    \label{tab:trace}
\end{longtable}

\begin{landscape}

\begin{table}
\vspace{-3cm}\hspace{-0.6cm}\begin{tabularx}{1.692\textwidth}{|p{1cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|}
    
    \hline
    & MM-1 & MM-2 & MM-3  & IG-1  & IG-2  & PM-1  & PM-2  & PM-3  & PV-1  & PV-2 & LB-1 & A-1 & A-2 & LF-1 & LF-2 & P-1 & P-2 & OE-1 & S-1 & C-1 & L-1\\
    \hline
  FR.1  & X & & & & & & & & & & & & & & & & & & & &\\\hline
  FR.2  & & & & X & & & & & & & & & & & & & & & & &\\\hline
  FR.3  & & & & X & & & & & & & & & & & & & & & & &\\\hline
  FR.4  & & & & X & & & & & & &  &  & & & & & & & & &\\\hline
  FR.5  & & & & X & & & & & &  & & & & & & & & & & &\\\hline
  FR.6  & & &  &  & X & & & &  & & & & & & & & & & & &\\\hline
  FR.7  & & &  &  & X & & & &  & & & & & & & & & & & &\\\hline
  FR.8 &  & & & X & & & & & & & &  & & & & & & & & &\\\hline
  FR.9  & & & & X & & & & & & & & & & & & & & & & &\\\hline
  FR.10  & & & & X & & & & & & & & & & & & & & & & &\\\hline
  FR.11  & & & & & & X & X & X & & & & & & & & & & & & &\\\hline
  FR.12  & & & & & & X & X & X & & & & & & & & & & & & &\\\hline
  FR.13  & &  & & & & & & & & & &X & & & & & & & & &\\\hline
  FR.14 &  &  & & & & & & & & & &  &X & & & & & & & &\\\hline
  FR.15  & & &  &  & X & & & &  & & & & & & & & & & & &\\\hline
  FR.16  & & &  &  & X & X & X & X &  & & & & & & & & & & & &\\\hline
  FR.17 &  & & &  & X & X & X & X & & &  & & & & & & & & & &\\\hline
  FR.18 &  & & X & & & &  &  &  & &  & & & & & & & & & &\\\hline
  FR.19 &  & X &  & & & &  &  &  & &  & & & & & & & & & &\\\hline
  FR.20 &  & &  & & & & & & &X &  & & & & & & & & & &\\\hline
  FR.21 &  &  &  & & & & & &X & & & &  & & & & & & & &\\\hline
  FR.22 &  &  &  & & & & & & &X & & &  & & & & & & & & \\\hline
  FR.23 &  &  &  & & & & & & &X & & &  & & & & & & & &\\\hline
  FR.24 &  &  &  & & & & & & & &X & &  & & & & & & & &\\\hline
  \end{tabularx}
    \caption{Traceability Matrix for Requirements and Tests Cases}
    \label{tab:trace2}
\end{table}
\end{landscape} 


% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

 \section{Unit Test Summary}
    
Unit testing will be done for all applicable modules, as defined in the \href{https://github.com/Tamas-Leung/CodeChamp/blob/main/docs/Design/MIS/MIS.pdf}{Module Interface Specification}. For back-end modules, Mocha will be used as the unit testing framework. For front-end modules, Jasmine will be used. In particular, for back-end testing, all API endpoints were tested. This covered operations of creating, reading, updating, and deleting data. The data we used to verify these operations were mocked data that was used by a mock database for the purposes of testing. As our project contains two distinct components (the backend and the frontend), the tests themselves will be placed alongside the code for the back-end in the \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/src/backend}{tests directory} tests directory as a separation of concerns. For front-end testing, components will be tested for each page. This involves testing the usability and functionality of menus, buttons, interactions, events and other GUI elements visible to the user using a Chrome runner. In accordance with the Angular convention, the test for each component will be placed in the same directory as the component itself as a `.spec.ts' file. The files for the frontend can be found \href{https://github.com/Tamas-Leung/CodeChamp/tree/main/src/frontend}{here}.

 \subsection{Unit Testing Scope}\label{scope}

Modules M32, M33, M34 are out of scope as they are not developed by the CodeChamp team. In particular, M32 deals with the environment used to compile and execute code, which will be handled by an external service. Likewise, M33, the database module, is out of scope as it's handled by an external service. Finally, the router module is out of scope as it is provided as an abstraction by the Angular framework and ultimately implemented by the browser used. As shown in the \href{https://github.com/Tamas-Leung/CodeChamp/blob/main/docs/Design/MG/MG.pdf}{Module Guide}, many of the modules which are in scope depend on these modules. In these cases, a mock will be used for the modules that are out of scope. For instance, if a module relies on some database operations such as insertions or retrieval of data, the mock will be used to fake those interactions instead.
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\begin{table}[H]
    \begin{tabularx}{\textwidth}{ | p{6cm} | p{6.85cm} | }
    \hline
    Symbolic Constant & Value \\
    \hline
    INPUT\_RESPONSE\_TIME & 2 seconds \\
    \hline
    BACKEND\_COMPILE\_TIME & 10 seconds \\
    \hline
    NUMBER\_OF\_ROUNDS & 3 \\
    \hline
    \end{tabularx}
    \caption{Symbolic Constants}
    \label{tab:trace}
\end{table}

\subsection{Usability Survey Questions}\label{UsabilitySurveyQuestions} 

% \wss{This is a section that would be appropriate for some projects.}
\begin{itemize}
    \item From 1 - 10, How easy was the interface to navigate? 
    1 being un-navigatable, 10 being no issues navigating.

    \item From 1 - 10, How consistent was the visual theming of the website? 1 being not consistent at all, 10 being super consistent. If users answered less than 8, testers should follow up and ask for feedback.

    \item Would you prefer the copy code button to copy the whole URL or just the game id? The team will conduct an A/B test in which half the users are tested on copying the URL then trying copying the game id and vice versa for the other group.

    \item What features do you feel are missing from the game?

    \item Which programming language should the platform support?


\end{itemize}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.  Please answer the following questions:

\begin{enumerate}
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage etc.  You should look to
  identify at least one item for each team member.
  
  \begin{enumerate}
    \item  \textbf{Jasmine (Frontend Unit Tests)}: Frontend of the system, implemented in Angular is to be tested using Jasmine. Jasmine allows modular testing allowing tests to be written for each component individually. Jasmine also allows state manipulation and monitoring of the systems outputs as the state changes. This will be a crucial part of the verification plan to verify the correctness of responsive user interface based on it's state.
    
    \item  \textbf{TestCafe (End-to-End)}: End to End testing is important to ensure the end user experiences correctly corresponds with the requirements. TestCafe works well as it is able to simulate user inputs onto a web page, mimicking a real user. This is a important as it allows building test plans for user scenarios.
    
    \item \textbf{Jest (Backend Unit Tests) - Youssef Rizkalla}: The backend of the system will be implemented in Node.js and will be tested using Jest. This will help ensure the correctness of the code-base and help identify regressions and weak-points for developers and reviewers, as it can track code coverage across the back-end. Ultimately, this will give the team a high-degree of confidence that the implementation matches the design and requirements needed for the project to succeed.
    
    \item  \textbf{CI testing - Zhiming Zhao}:
    Continuous integration testing focuses on execution during the continuous integration process. Testing in the CI process allows for rapid feedback and stops the progression of the artifact if minimum quality is not met. CI testing has the benefit of repeatability, it's also able to run builds or tests in parallel with other team members. To summarize, CI testing allows for iteration and rapid feedback, it also saves the team a lot of time since a failing build is much less severe than a failing deployment.
    
    \item \textbf{Performance testing (Postman/Chrome dev tools) - Tamas Leung}: Performance testing is important to ensure website is to common web standards. Skills to identify common performance issues such as large CSS files or back end algorithms are important to ensure websites are snappy and are a smooth experience for users.
    
\end{enumerate}

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
  
  \begin{enumerate}
\item \textbf{Jasmine (Frontend Unit Tests) - Anton Kanugalawattage} This skill could be acquired by reading and following examples of existing implementation of tests using this technology. Another approach to acquire this skill is to read the documentation or blogs of how this testing technology is used in systems (software testing classes, technology company's engineering blogs). Anton will pursue this skill as he has worked on a project which has utilized a similar testing technology before. Also, as he is the front-end lead this verifying components and changes to the front-end will be a crucial part as he will also we reviewing these changes.

\item  \textbf{Performance testing - Tamas Leung}: In order to learn performance testing, Tamas will study the common issues faced when doing performance testing. For front-end, the google developer tools can generate a lighthouse report in which shows the best practices for web developoment. Tamas will use these reports to learn and understand how to improve the software to run at the webs standards. For back-end, Tamas will study common issues with building heavy load backend systems and will watch videos on methods to improve the servers such as load balancing.

\item \textbf{Jest (Backend Unit Tests) - Youssef Rizkalla}: This can be developed by observing and critiquing open-source projects and identifying the testing techniques utilized. Youssef will work on this as he is the back-end lead and main code-reviewer, so it is important to understand testing patterns that will maintain good code health. Observing open-source projects allows us to identify issues that have occurred in other projects and develop a design to get around them. Additionally, testing patterns can be learned from official documentation or trustworthy blog posts that are published by professional Software Engineers. This allows us to learn from engineers who developed the tooling in the first place, which will allow us to understand and set best practices in our code-base. When reviewing code, Youssef can ensure that others are submitting unit tests according to the standards set, and that no regressions are occurring from new pull requests.

\item \textbf{TestCafe (End-to-End)  - Dipendra Subedi}: This skill could be developed by reading web tutorials, video examples, and seeing the use of TestCafe in existing open source projects. Web tutorials run through specific examples of how to use TestCafe, and similar code structure shall be utilized when developing end to end tests for CodeChamp. Open source projects will also be useful since previously identified issues can be mitigated when implemented for CodeChamp. As the design lead, Dipendra is knowledge-able in the design of the entire web application, so developing this skill will ensure that CodeChamp simulates what a real use case scenario looks like from start to finish.

\item \textbf{CI Testing - Zhiming Zhao}: To start learning CI testing, it's better to start small and catch easiest errors first. Some of the simplest and easiest to fix errors can end up causing the biggest problems if they make it into production workloads. It's a good choice to configure some automatic testing to take place on developer machines before code is committed. The second approach would be built on top of first, and it would be making security a part of CI testing. Catching security issues early in the software development life-cycle means they are much easier, cheaper and safer to fix. Adding some basic static code analysis tools and dependency checkers can vastly improve the security posture of an application by providing fast feedback and early detection of common security problems and potential vulnerabilities. Since approach 2 builds on approach 1, it's fair to start learning by starting small and then builds on to approach 2 by implementing security into the CI testing. 

  \end{enumerate}
  
\end{enumerate}

\end{document}